##  ğŸš€ Deep Q-Network Lunar Lander Notebook ğŸŒ™

## Overview ğŸ§ âœ¨

This repository contains a Jupyter Notebook implementation of a **Deep Q-Network (DQN)** to solve the ğŸŒŒ **Lunar Lander** environment from OpenAI's Gym. The goal of this project is to apply **reinforcement learning (RL)** techniques to train an agent capable of safely landing a spaceship ğŸ›¸ on a designated pad ğŸŸ© within the environment. 

---

## Features ğŸ’¡

- **Deep Q-Learning:** ğŸ“ˆ Utilizes a neural network to approximate Q-values for state-action pairs and updates them iteratively using the **Bellman equation**.
- **Replay Buffer:** ğŸ’¾ Implements experience replay to store and sample past experiences for stable training.
- **Epsilon-Greedy Policy:** ğŸ² Balances exploration and exploitation using a decaying epsilon-greedy policy.
- **Double Q-Networks:** ğŸ”— Uses separate online and target networks to stabilize the learning process.
- **Training Loop:** ğŸ”„ Incorporates a robust training loop with episodic evaluations and the ability to save rendered episodes as videos ğŸ¥.

---




## Prerequisites ğŸ› ï¸

To run the notebook, ensure you have the following:

- **Python 3.x** ğŸ
- **Jupyter Notebook** ğŸ““
- Libraries:
  - `gym` ğŸ‹ï¸â€â™€ï¸
  - `jax` âš¡
  - `jaxlib` âš¡
  - `dm-haiku` ğŸ§±
  - `optax` ğŸ§®
  - `numpy` ğŸ”¢
  - `matplotlib` ğŸ“Š
  - `chex` âœ…

Install the required dependencies using:

```bash
pip install -r requirements.txt
```

---

## Notebook Structure ğŸ“‚

1. **Setup ğŸ› ï¸:**
   - Installs and imports all required packages.
   - Initializes the **Lunar Lander** environment from OpenAI Gym.

2. **Q-Learning Theory ğŸ“š:**
   - Explains key concepts like the **Bellman equation**, **greedy action selection**, and **Q-value updates**.

3. **Model Implementation ğŸ§‘â€ğŸ’»:**
   - Builds the neural network using `dm-haiku` for Q-function approximation.
   - Implements functions for computing **squared loss**, **Bellman targets**, and **Q-learning loss**.

4. **Replay Buffer ğŸ’¾:**
   - Creates a replay buffer to store transitions (`state`, `action`, `reward`, `next_state`, `done`).

5. **Training Loop ğŸ”„:**
   - Trains the agent using a **policy-decay mechanism** (`epsilon-greedy`) and periodic updates of the target network.

6. **Evaluation ğŸ¯:**
   - Logs training performance and plots the **episode rewards** to visualize learning progress.

---

## Key Functions ğŸ”‘

### Action Selection ğŸ²
- **Greedy Action Selection:** Chooses the action with the highest Q-value.
- **Random Action Selection:** Selects a random action for exploration.
- **Epsilon-Greedy Policy:** Combines greedy and random selection based on an epsilon schedule.

### Training ğŸ‹ï¸â€â™‚ï¸
- **Loss Function:** Calculates the Q-learning loss using **squared error** between predicted Q-values and Bellman targets.
- **Optimizer:** Updates network parameters using `optax.adam`.

---

## Running the Notebook â–¶ï¸

Open **`luna-lander.ipynb`** and run the cells sequentially.

---

## Results ğŸ†

- The agent starts with **random actions** ğŸ² and gradually learns to control the spaceship ğŸš€.
- Training episodes demonstrate improvement in **landing accuracy** ğŸŸ¢ as the model optimizes the Q-function.
- Final evaluations show the agent's ability to **land consistently** with high reward scores ğŸŒŸ.
https://github.com/user-attachments/assets/1cb1f589-d32d-4756-816d-79e7fb917222

---

## Contributions ğŸ¤

This project was created as part of the **African Institute for Mathematical Sciences (AIMS)** program on Reinforcement Learning (2024) ğŸ«, led by **Arnu Pretorius**, Staff Research Scientist at InstaDeep ğŸ§ª. It is adapted from the **Deep Learning Indaba 2022** materials ğŸ“.

---

## License ğŸ“œ

This project is licensed under the **Apache License 2.0**. ğŸ“„
